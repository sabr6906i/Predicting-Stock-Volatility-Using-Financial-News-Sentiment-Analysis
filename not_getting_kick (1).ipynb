{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxM9wYIfCr1g"
      },
      "source": [
        "#Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBkUN7kmDGjU"
      },
      "source": [
        "## Installation of library for data pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ES8NhfUCfkS",
        "outputId": "07f7a0de-0e3c-4ec3-d0fa-f638aed4b1d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: duckdb in c:\\users\\sarbjeet singh pal\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (1.4.3)\n",
            "Requirement already satisfied: pandas in c:\\users\\sarbjeet singh pal\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\sarbjeet singh pal\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from pandas) (2.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sarbjeet singh pal\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sarbjeet singh pal\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sarbjeet singh pal\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\sarbjeet singh pal\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install duckdb pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG951KHYDK6Y"
      },
      "source": [
        "## Data pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "1f3962d0217244a0a1703483a169ea0a",
            "12985758a2234fe9b7ee8d96a2184d2e",
            "87127f8de65646028d58693b8e45cad5"
          ]
        },
        "id": "gFQOCd-IDRu9",
        "outputId": "ec5d8721-5b04-4c56-b0c8-c2865aec1cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching news for ticker: CAT\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4551f49c5e94d478ccecf3fde5f53f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUCCESS: 495 articles saved → CAT_yahoo_finance_news_5yr.json\n"
          ]
        }
      ],
      "source": [
        "import duckdb\n",
        "import json\n",
        "from datetime import datetime\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "TICKER = \"CAT\"        # <<< CHANGE THIS\n",
        "YEARS_BACK = 5\n",
        "OUTPUT_FILE = f\"{TICKER}_yahoo_finance_news_5yr.json\"\n",
        "\n",
        "PARQUET_URL = (\n",
        "    \"https://huggingface.co/datasets/\"\n",
        "    \"bwzheng2010/yahoo-finance-data/resolve/main/data/stock_news.parquet\"\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# DATE CUTOFF\n",
        "# -----------------------------\n",
        "cutoff_year = datetime.now().year - YEARS_BACK\n",
        "cutoff_date = f\"{cutoff_year}-01-01\"\n",
        "\n",
        "# -----------------------------\n",
        "# QUERY\n",
        "# -----------------------------\n",
        "print(f\"Fetching news for ticker: {TICKER}\")\n",
        "\n",
        "con = duckdb.connect()\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "    report_date,\n",
        "    title,\n",
        "    publisher,\n",
        "    related_symbols,\n",
        "    type,\n",
        "    link\n",
        "FROM read_parquet('{PARQUET_URL}')\n",
        "WHERE\n",
        "    report_date::DATE >= DATE '{cutoff_date}'\n",
        "    AND list_contains(string_split(related_symbols::VARCHAR, ','), '{TICKER}')\n",
        "ORDER BY report_date DESC\n",
        "\"\"\"\n",
        "\n",
        "df = con.execute(query).fetchdf()\n",
        "\n",
        "# -----------------------------\n",
        "# BUILD FEED\n",
        "# -----------------------------\n",
        "feed = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    symbols_data = row[\"related_symbols\"]\n",
        "    # Convert numpy arrays to lists for JSON serialization\n",
        "    if isinstance(symbols_data, np.ndarray):\n",
        "        symbols_data = symbols_data.tolist()\n",
        "    # Ensure it's a list even if it was None or another non-iterable type\n",
        "    elif symbols_data is None:\n",
        "        symbols_data = []\n",
        "    # If it's already a list, it remains a list\n",
        "\n",
        "    feed.append({\n",
        "        \"date\": str(row[\"report_date\"]),\n",
        "        \"ticker\": TICKER,\n",
        "        \"title\": row[\"title\"],\n",
        "        \"publisher\": row[\"publisher\"],\n",
        "        \"symbols\": symbols_data,\n",
        "        \"type\": row[\"type\"],\n",
        "        \"url\": row[\"link\"]\n",
        "    })\n",
        "\n",
        "# -----------------------------\n",
        "# SAVE\n",
        "# -----------------------------\n",
        "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(feed, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"SUCCESS: {len(feed)} articles saved → {OUTPUT_FILE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEuogKnsDnvT"
      },
      "source": [
        "## Conversion of data into xml/feeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncMjnhO-DnZp",
        "outputId": "4c2a7c25-2458-4957-ff4d-243e12b89610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUCCESS: RSS feed written to CAT.rss.xml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Sarbjeet Singh Pal\\AppData\\Local\\Temp\\ipykernel_33588\\1719646977.py:39: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ET.SubElement(channel, \"lastBuildDate\").text = format_datetime(datetime.utcnow())\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Convert Yahoo Finance JSON news to Yahoo-style RSS feed\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from datetime import datetime\n",
        "from email.utils import format_datetime\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "TICKER = \"CAT\"\n",
        "INPUT_JSON = f\"{TICKER}_yahoo_finance_news_5yr.json\"\n",
        "\n",
        "OUTPUT_RSS = f\"{TICKER}.rss.xml\"\n",
        "\n",
        "CHANNEL_TITLE = f\"{TICKER} News – Yahoo Finance\"\n",
        "CHANNEL_LINK = f\"https://finance.yahoo.com/quote/{TICKER}\"\n",
        "CHANNEL_DESCRIPTION = f\"Latest news headlines for {TICKER}\"\n",
        "LANGUAGE = \"en-us\"\n",
        "\n",
        "# -----------------------------\n",
        "# LOAD JSON\n",
        "# -----------------------------\n",
        "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    news_items = json.load(f)\n",
        "\n",
        "# -----------------------------\n",
        "# BUILD RSS\n",
        "# -----------------------------\n",
        "rss = ET.Element(\"rss\", version=\"2.0\")\n",
        "channel = ET.SubElement(rss, \"channel\")\n",
        "\n",
        "ET.SubElement(channel, \"title\").text = CHANNEL_TITLE\n",
        "ET.SubElement(channel, \"link\").text = CHANNEL_LINK\n",
        "ET.SubElement(channel, \"description\").text = CHANNEL_DESCRIPTION\n",
        "ET.SubElement(channel, \"language\").text = LANGUAGE\n",
        "ET.SubElement(channel, \"lastBuildDate\").text = format_datetime(datetime.utcnow())\n",
        "\n",
        "# -----------------------------\n",
        "# ADD ITEMS\n",
        "# -----------------------------\n",
        "for item in news_items:\n",
        "    rss_item = ET.SubElement(channel, \"item\")\n",
        "\n",
        "    ET.SubElement(rss_item, \"title\").text = item[\"title\"]\n",
        "    ET.SubElement(rss_item, \"link\").text = item[\"url\"]\n",
        "\n",
        "    guid = ET.SubElement(rss_item, \"guid\", isPermaLink=\"true\")\n",
        "    guid.text = item[\"url\"]\n",
        "\n",
        "    # Convert YYYY-MM-DD → RFC 2822\n",
        "    pub_date = datetime.strptime(item[\"date\"], \"%Y-%m-%d\")\n",
        "    ET.SubElement(rss_item, \"pubDate\").text = format_datetime(pub_date)\n",
        "\n",
        "    source = ET.SubElement(rss_item, \"source\")\n",
        "    source.text = item[\"publisher\"]\n",
        "\n",
        "# -----------------------------\n",
        "# WRITE FILE\n",
        "# -----------------------------\n",
        "tree = ET.ElementTree(rss)\n",
        "tree.write(OUTPUT_RSS, encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "print(f\"SUCCESS: RSS feed written to {OUTPUT_RSS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1M5Ij3FD4Tu"
      },
      "source": [
        "## Data cleaning and formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhjBVSqmELZl",
        "outputId": "9a2bff1a-a394-49ef-a471-78a33e7d0fc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing complete_data_collection.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile complete_data_collection.py\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "ticker = \"CAT\"\n",
        "\n",
        "def collect_news_data(ticker='CAT', max_items=5000):\n",
        "    \"\"\"\n",
        "    Reads news data from a local RSS feed file (.rss.xml).\n",
        "    \"\"\"\n",
        "    print(f\"Collecting news data from local file '{ticker}.rss.xml'...\")\n",
        "\n",
        "    rss_file_name = f\"{ticker}.rss.xml\" # Assuming the user uploaded this file\n",
        "\n",
        "    try:\n",
        "        # Check if the file exists\n",
        "        if not os.path.exists(rss_file_name):\n",
        "            print(f\"Error: RSS file '{rss_file_name}' not found in the current directory. Please upload it.\")\n",
        "            return pd.DataFrame(columns=['Source', 'Headline', 'PubDate'])\n",
        "\n",
        "        # Read the content of the local RSS file\n",
        "        with open(rss_file_name, 'r', encoding='utf-8') as f:\n",
        "            rss_content = f.read()\n",
        "\n",
        "        soup = BeautifulSoup(rss_content, \"xml\") # Parse the content\n",
        "        items = soup.find_all(\"item\")\n",
        "\n",
        "        data = []\n",
        "        for i, item in enumerate(items[:max_items], 1):\n",
        "            try:\n",
        "                pub_date = item.pubDate.text if item.pubDate else None\n",
        "                headline = item.title.text if item.title else None\n",
        "                source = item.link.text if item.link else None\n",
        "\n",
        "                data.append({\n",
        "                    'Source': source,\n",
        "                    'Headline': headline,\n",
        "                    'PubDate': pub_date\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing item {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        print(f\"Successfully collected {len(df)} news items from '{rss_file_name}'\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error collecting news data from local file '{rss_file_name}': {e}\")\n",
        "        return pd.DataFrame(columns=['Source', 'Headline', 'PubDate'])\n",
        "\n",
        "\n",
        "def clean_news_data(df):\n",
        "    \"\"\"\n",
        "    Clean and process news data\n",
        "    \"\"\"\n",
        "    print(\"Cleaning news data...\")\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"Warning: Empty dataframe provided\")\n",
        "        return df\n",
        "\n",
        "    # Parse publication dates\n",
        "    df['PubDate_Clean'] = df['PubDate'].str.split(' ').str[1:4].str.join(\" \")\n",
        "    df['PubDate'] = pd.to_datetime(df['PubDate_Clean'], format=\"%d %b %Y\", errors='coerce')\n",
        "    df['Date'] = df['PubDate'].dt.date\n",
        "\n",
        "    # Calculate headline length\n",
        "    df['Headline Length'] = df['Headline'].str.len()\n",
        "\n",
        "    # Drop temporary column\n",
        "    df.drop('PubDate_Clean', axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "    print(f\"Cleaned data: {len(df)} rows with dates from {df['Date'].min()} to {df['Date'].max()}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def collect_stock_data(ticker='CAT', period='5y'):\n",
        "    \"\"\"\n",
        "    Collect stock price data using yfinance\n",
        "    \"\"\"\n",
        "    print(f\"Collecting stock data for {ticker}...\")\n",
        "\n",
        "    try:\n",
        "        stock = yf.Ticker(ticker)\n",
        "        df = stock.history(period=period)\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"Warning: No stock data retrieved\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Select relevant columns\n",
        "        df_stock = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].copy()\n",
        "\n",
        "        # Add date column\n",
        "        df_stock.index = pd.to_datetime(df_stock.index)\n",
        "        df_stock['Date'] = df_stock.index.date\n",
        "        df_stock.index = df_stock.index.date\n",
        "\n",
        "        print(f\"Successfully collected {len(df_stock)} days of stock data\")\n",
        "        return df_stock\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error collecting stock data: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "def merge_datasets(stock_df, news_df):\n",
        "    \"\"\"\n",
        "    Merge stock and news data on date\n",
        "    \"\"\"\n",
        "    print(\"Merging datasets...\")\n",
        "\n",
        "    if stock_df.empty or news_df.empty:\n",
        "        print(\"Warning: One or both dataframes are empty\")\n",
        "        if not stock_df.empty:\n",
        "            return stock_df\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Perform left merge to keep all trading days\n",
        "    merged_df = stock_df.merge(news_df, on='Date', how='left')\n",
        "\n",
        "    print(f\"Merged dataset: {len(merged_df)} rows\")\n",
        "    print(f\"News coverage: {merged_df['Headline'].notna().sum()} days have news\")\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "\n",
        "def save_datasets(news_raw, news_clean, stock_data, merged_data):\n",
        "    \"\"\"\n",
        "    Save all datasets to CSV files\n",
        "    \"\"\"\n",
        "    print(\"\\nSaving datasets...\")\n",
        "\n",
        "    try:\n",
        "        if not news_raw.empty:\n",
        "            news_raw.to_csv('News_raw.csv', index=False)\n",
        "            print(\"✓ Saved News_raw.csv\")\n",
        "\n",
        "        if not news_clean.empty:\n",
        "            news_clean.to_csv('news_cleaned.csv', index=False)\n",
        "            print(\"✓ Saved news_cleaned.csv\")\n",
        "\n",
        "        if not stock_data.empty:\n",
        "            stock_data.to_csv('stock_data.csv')\n",
        "            print(\"✓ Saved stock_data.csv\")\n",
        "\n",
        "        if not merged_data.empty:\n",
        "            merged_data.to_csv('merged_medsem_data.csv', index=False)\n",
        "            print(\"✓ Saved merged_medsem_data.csv\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving files: {e}\")\n",
        "\n",
        "\n",
        "def generate_summary_statistics(merged_df):\n",
        "    \"\"\"\n",
        "    Generate summary statistics for the report\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DATASET SUMMARY STATISTICS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if merged_df.empty:\n",
        "        print(\"No data to summarize\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nDate Range: {merged_df['Date'].min()} to {merged_df['Date'].max()}\")\n",
        "    print(f\"Total Trading Days: {len(merged_df)}\")\n",
        "    print(f\"Days with News: {merged_df['Headline'].notna().sum()}\")\n",
        "    print(f\"Days without News: {merged_df['Headline'].isna().sum()}\")\n",
        "\n",
        "    print(f\"\\nStock Price Statistics:\")\n",
        "    print(f\"  Opening Price Range: ${merged_df['Open'].min():.2f} - ${merged_df['Open'].max():.2f}\")\n",
        "    print(f\"  Closing Price Range: ${merged_df['Close'].min():.2f} - ${merged_df['Close'].max():.2f}\")\n",
        "    print(f\"  Average Daily Volume: {merged_df['Volume'].mean():,.0f}\")\n",
        "\n",
        "    if 'Headline Length' in merged_df.columns:\n",
        "        print(f\"\\nNews Statistics:\")\n",
        "        print(f\"  Average Headline Length: {merged_df['Headline Length'].mean():.0f} characters\")\n",
        "        print(f\"  Shortest Headline: {merged_df['Headline Length'].min():.0f} characters\")\n",
        "        print(f\"  Longest Headline: {merged_df['Headline Length'].max():.0f} characters\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"WiDS 2025 - Stock Market & News Sentiment Analysis\")\n",
        "    print(\"Data Collection Script\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Configuration\n",
        "    TICKER = 'CAT'\n",
        "    PERIOD = '5y'  # Can change to '3mo', '6mo', etc.\n",
        "\n",
        "    # Step 1: Collect news data\n",
        "    news_raw = collect_news_data(ticker=TICKER, max_items=5000)\n",
        "\n",
        "    # Step 2: Clean news data\n",
        "    news_clean = clean_news_data(news_raw.copy()) if not news_raw.empty else pd.DataFrame()\n",
        "\n",
        "    # Step 3: Collect stock data\n",
        "    stock_data = collect_stock_data(ticker=TICKER, period=PERIOD) # Corrected variable name\n",
        "\n",
        "    # Step 4: Merge datasets\n",
        "    merged_data = merge_datasets(stock_data, news_clean)\n",
        "\n",
        "    # Step 5: Save all datasets\n",
        "    save_datasets(news_raw, news_clean, stock_data, merged_data)\n",
        "\n",
        "    # Step 6: Generate summary\n",
        "    generate_summary_statistics(merged_data)\n",
        "\n",
        "    print(\"\\n✓ Data collection complete!\")\n",
        "    print(\"\\nGenerated files:\")\n",
        "    print(\"  - News_raw.csv\")\n",
        "    print(\"  - news_cleaned.csv\")\n",
        "    print(\"  - stock_data.csv\")\n",
        "    print(\"  - merged_medsem_data.csv\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM7_ZkRZFALx"
      },
      "source": [
        "# ML models and features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cxo9NakE_XW",
        "outputId": "0fa1943a-7a73-4663-e134-eb5e73b8ddaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing week4_sentiment_features.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile week4_sentiment_features.py\n",
        "\n",
        "\"\"\"\n",
        "Week 4: Sentiment Analysis & Feature Engineering\n",
        "WiDS 2025 Project - Stock Volatility Prediction\n",
        "\n",
        "This script:\n",
        "1. Performs sentiment analysis on news headlines\n",
        "2. Engineers features from sentiment scores\n",
        "3. Prepares data for machine learning models\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SentimentAnalyzer:\n",
        "    \"\"\"Analyze sentiment of financial news headlines\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "    def get_sentiment_scores(self, text):\n",
        "        \"\"\"\n",
        "        Get VADER sentiment scores for text\n",
        "        Returns: compound, positive, negative, neutral scores\n",
        "        \"\"\"\n",
        "        if pd.isna(text):\n",
        "            return 0.0, 0.0, 0.0, 0.0\n",
        "\n",
        "        scores = self.analyzer.polarity_scores(str(text))\n",
        "        return (\n",
        "            scores['compound'],\n",
        "            scores['pos'],\n",
        "            scores['neg'],\n",
        "            scores['neu']\n",
        "        )\n",
        "\n",
        "    def classify_sentiment(self, compound_score):\n",
        "        \"\"\"\n",
        "        Classify sentiment as Positive, Negative, or Neutral\n",
        "        Based on compound score thresholds\n",
        "        \"\"\"\n",
        "        if compound_score >= 0.05:\n",
        "            return 'Positive'\n",
        "        elif compound_score <= -0.05:\n",
        "            return 'Negative'\n",
        "        else:\n",
        "            return 'Neutral'\n",
        "\n",
        "\n",
        "class FeatureEngineer:\n",
        "    \"\"\"Engineer features for stock prediction\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_price_change(df):\n",
        "        \"\"\"Calculate daily price change and direction\"\"\"\n",
        "        df['Price_Change'] = df['Close'] - df['Open']\n",
        "        df['Price_Change_Pct'] = (df['Price_Change'] / df['Open']) * 100\n",
        "        df['Price_Direction'] = df['Price_Change'].apply(\n",
        "            lambda x: 1 if x > 0 else 0\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_volatility(df, window=5):\n",
        "        \"\"\"Calculate rolling volatility\"\"\"\n",
        "        df['Volatility'] = df['Close'].pct_change().rolling(window=window).std()\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_momentum(df):\n",
        "        \"\"\"Calculate price momentum indicators\"\"\"\n",
        "        df['Momentum_1d'] = df['Close'].pct_change(1)\n",
        "        df['Momentum_3d'] = df['Close'].pct_change(3)\n",
        "        df['Momentum_5d'] = df['Close'].pct_change(5)\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_ma(df, windows=[5, 10]):\n",
        "        \"\"\"Calculate moving averages\"\"\"\n",
        "        for window in windows:\n",
        "            df[f'MA_{window}'] = df['Close'].rolling(window=window).mean()\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def aggregate_daily_sentiment(df):\n",
        "        \"\"\"\n",
        "        Aggregate sentiment scores for days with multiple news\n",
        "        \"\"\"\n",
        "        sentiment_agg = df.groupby('Date').agg({\n",
        "            'Sentiment_Compound': ['mean', 'std', 'count'],\n",
        "            'Sentiment_Positive': 'mean',\n",
        "            'Sentiment_Negative': 'mean',\n",
        "            'Sentiment_Neutral': 'mean'\n",
        "        })\n",
        "\n",
        "        # Flatten column names\n",
        "        sentiment_agg.columns = [\n",
        "            'Sentiment_Mean', 'Sentiment_Std', 'News_Count',\n",
        "            'Positive_Mean', 'Negative_Mean', 'Neutral_Mean'\n",
        "        ]\n",
        "\n",
        "        # Fill std with 0 for days with single news\n",
        "        sentiment_agg['Sentiment_Std'] = sentiment_agg['Sentiment_Std'].fillna(0)\n",
        "\n",
        "        return sentiment_agg\n",
        "\n",
        "\n",
        "def add_sentiment_features(news_df):\n",
        "    \"\"\"\n",
        "    Add sentiment analysis features to news dataframe\n",
        "\n",
        "    Args:\n",
        "        news_df: DataFrame with 'Headline' column\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with sentiment features added\n",
        "    \"\"\"\n",
        "    print(\"Performing sentiment analysis...\")\n",
        "\n",
        "    analyzer = SentimentAnalyzer()\n",
        "\n",
        "    # Apply sentiment analysis to each headline\n",
        "    sentiment_results = news_df['Headline'].apply(\n",
        "        lambda x: analyzer.get_sentiment_scores(x)\n",
        "    )\n",
        "\n",
        "    # Unpack results into separate columns\n",
        "    news_df['Sentiment_Compound'] = sentiment_results.apply(lambda x: x[0])\n",
        "    news_df['Sentiment_Positive'] = sentiment_results.apply(lambda x: x[1])\n",
        "    news_df['Sentiment_Negative'] = sentiment_results.apply(lambda x: x[2])\n",
        "    news_df['Sentiment_Neutral'] = sentiment_results.apply(lambda x: x[3])\n",
        "\n",
        "    # Add sentiment classification\n",
        "    news_df['Sentiment_Label'] = news_df['Sentiment_Compound'].apply(\n",
        "        analyzer.classify_sentiment\n",
        "    )\n",
        "\n",
        "    print(f\"✓ Sentiment analysis complete for {len(news_df)} headlines\")\n",
        "\n",
        "    # Print sentiment distribution\n",
        "    print(\"\\nSentiment Distribution:\")\n",
        "    print(news_df['Sentiment_Label'].value_counts())\n",
        "    print(f\"\\nAverage Sentiment Score: {news_df['Sentiment_Compound'].mean():.3f}\")\n",
        "\n",
        "    return news_df\n",
        "\n",
        "\n",
        "def engineer_stock_features(stock_df):\n",
        "    \"\"\"\n",
        "    Add technical indicators and features to stock data\n",
        "\n",
        "    Args:\n",
        "        stock_df: DataFrame with OHLCV data\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with engineered features\n",
        "    \"\"\"\n",
        "    print(\"\\nEngineering stock features...\")\n",
        "\n",
        "    engineer = FeatureEngineer()\n",
        "\n",
        "    # Price-based features\n",
        "    stock_df = engineer.calculate_price_change(stock_df)\n",
        "    stock_df = engineer.calculate_volatility(stock_df)\n",
        "    stock_df = engineer.calculate_momentum(stock_df)\n",
        "    stock_df = engineer.calculate_ma(stock_df, windows=[5, 10])\n",
        "\n",
        "    # Volume features\n",
        "    stock_df['Volume_MA5'] = stock_df['Volume'].rolling(window=5).mean()\n",
        "    stock_df['Volume_Change'] = stock_df['Volume'].pct_change()\n",
        "\n",
        "    print(f\"✓ Engineered {len([c for c in stock_df.columns if c not in ['Open', 'High', 'Low', 'Close', 'Volume', 'Date']])} new features\")\n",
        "\n",
        "    return stock_df\n",
        "\n",
        "\n",
        "def create_ml_dataset(stock_df, news_df):\n",
        "    \"\"\"\n",
        "    Merge stock and sentiment data, create features for ML\n",
        "\n",
        "    Args:\n",
        "        stock_df: DataFrame with stock features\n",
        "        news_df: DataFrame with sentiment features\n",
        "\n",
        "    Returns:\n",
        "        DataFrame ready for machine learning\n",
        "    \"\"\"\n",
        "    print(\"\\nCreating ML-ready dataset...\")\n",
        "\n",
        "    # Ensure Date columns are datetime\n",
        "    if 'Date' not in stock_df.columns and stock_df.index.name == 'Date':\n",
        "        stock_df = stock_df.reset_index()\n",
        "    if 'Date' in news_df.columns:\n",
        "        news_df['Date'] = pd.to_datetime(news_df['Date'])\n",
        "    stock_df['Date'] = pd.to_datetime(stock_df['Date'])\n",
        "\n",
        "    # Aggregate sentiment by date\n",
        "    engineer = FeatureEngineer()\n",
        "    sentiment_agg = engineer.aggregate_daily_sentiment(news_df)\n",
        "\n",
        "    # Merge with stock data\n",
        "    ml_df = stock_df.merge(sentiment_agg, left_on='Date', right_index=True, how='left')\n",
        "\n",
        "    # Fill missing sentiment values (days without news)\n",
        "    sentiment_cols = ['Sentiment_Mean', 'Sentiment_Std', 'News_Count',\n",
        "                     'Positive_Mean', 'Negative_Mean', 'Neutral_Mean']\n",
        "    ml_df[sentiment_cols] = ml_df[sentiment_cols].fillna(0)\n",
        "\n",
        "    # Create lagged features (previous day sentiment)\n",
        "    ml_df['Sentiment_Lag1'] = ml_df['Sentiment_Mean'].shift(1)\n",
        "    ml_df['News_Count_Lag1'] = ml_df['News_Count'].shift(1)\n",
        "\n",
        "    # Create target variable (next day price direction)\n",
        "    ml_df['Target_Next_Day'] = ml_df['Price_Direction'].shift(-1)\n",
        "\n",
        "    # Drop rows with NaN in target or critical features\n",
        "    ml_df = ml_df.dropna(subset=['Target_Next_Day', 'Volatility', 'MA_5'])\n",
        "\n",
        "    print(f\"✓ Created dataset with {len(ml_df)} samples and {len(ml_df.columns)} features\")\n",
        "    print(f\"✓ Target distribution: {ml_df['Target_Next_Day'].value_counts().to_dict()}\")\n",
        "\n",
        "    return ml_df\n",
        "\n",
        "\n",
        "def save_processed_data(news_df, stock_df, ml_df):\n",
        "    \"\"\"Save all processed datasets\"\"\"\n",
        "    print(\"\\nSaving processed datasets...\")\n",
        "\n",
        "    news_df.to_csv('news_with_sentiment.csv', index=False)\n",
        "    print(\"✓ Saved news_with_sentiment.csv\")\n",
        "\n",
        "    stock_df.to_csv('stock_with_features.csv', index=False)\n",
        "    print(\"✓ Saved stock_with_features.csv\")\n",
        "\n",
        "    ml_df.to_csv('ml_ready_dataset.csv', index=False)\n",
        "    print(\"✓ Saved ml_ready_dataset.csv\")\n",
        "\n",
        "\n",
        "def generate_feature_summary(ml_df):\n",
        "    \"\"\"Generate summary statistics of features\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FEATURE ENGINEERING SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"\\nDataset Shape: {ml_df.shape}\")\n",
        "    print(f\"Date Range: {ml_df['Date'].min()} to {ml_df['Date'].max()}\")\n",
        "\n",
        "    print(\"\\nPrice Statistics:\")\n",
        "    print(f\"  Average Daily Return: {ml_df['Price_Change_Pct'].mean():.2f}%\")\n",
        "    print(f\"  Volatility (std): {ml_df['Volatility'].mean():.4f}\")\n",
        "    print(f\"  Up Days: {(ml_df['Price_Direction'] == 1).sum()}\")\n",
        "    print(f\"  Down Days: {(ml_df['Price_Direction'] == 0).sum()}\")\n",
        "\n",
        "    print(\"\\nSentiment Statistics:\")\n",
        "    print(f\"  Average Sentiment: {ml_df['Sentiment_Mean'].mean():.3f}\")\n",
        "    print(f\"  Days with News: {(ml_df['News_Count'] > 0).sum()}\")\n",
        "    print(f\"  Average News per Day: {ml_df['News_Count'].mean():.1f}\")\n",
        "\n",
        "    print(\"\\nFeature Correlation with Target:\")\n",
        "    corr_cols = ['Sentiment_Mean', 'Price_Change_Pct', 'Volatility',\n",
        "                 'Momentum_1d', 'Volume_Change']\n",
        "    correlations = ml_df[corr_cols + ['Target_Next_Day']].corr()['Target_Next_Day'].drop('Target_Next_Day')\n",
        "    for feature, corr in correlations.items():\n",
        "        print(f\"  {feature}: {corr:.3f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Week 4: Sentiment Analysis & Feature Engineering\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Load data\n",
        "    try:\n",
        "        news_df = pd.read_csv('news_cleaned.csv')\n",
        "        stock_df = pd.read_csv('stock_data.csv')\n",
        "        print(f\"✓ Loaded {len(news_df)} news items\")\n",
        "        print(f\"✓ Loaded {len(stock_df)} trading days\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Please run the data collection script first!\")\n",
        "        return\n",
        "\n",
        "    # Step 1: Add sentiment features\n",
        "    news_df = add_sentiment_features(news_df)\n",
        "\n",
        "    # Step 2: Engineer stock features\n",
        "    stock_df = engineer_stock_features(stock_df)\n",
        "\n",
        "    # Step 3: Create ML dataset\n",
        "    ml_df = create_ml_dataset(stock_df, news_df)\n",
        "\n",
        "    # Step 4: Generate summary\n",
        "    generate_feature_summary(ml_df)\n",
        "\n",
        "    # Step 5: Save all processed data\n",
        "    save_processed_data(news_df, stock_df, ml_df)\n",
        "\n",
        "    print(\"\\n✓ Week 4 Feature Engineering Complete!\")\n",
        "    print(\"\\nNext step: Build prediction models using 'ml_ready_dataset.csv'\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Install vader if not present\n",
        "    try:\n",
        "        from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "    except ImportError:\n",
        "        print(\"Installing vaderSentiment...\")\n",
        "        import subprocess\n",
        "        import sys\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'vaderSentiment'])\n",
        "        print(\"✓ Installed vaderSentiment\")\n",
        "\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X2Y5QO9GI13",
        "outputId": "e64c0fc8-92dc-4d00-af0c-70718c33cbbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing week4_ml_modeling.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile week4_ml_modeling.py\n",
        "\n",
        "\"\"\"\n",
        "Week 4: Machine Learning Models for Stock Prediction\n",
        "WiDS 2025 Project\n",
        "\n",
        "This script builds and evaluates multiple ML models:\n",
        "- Logistic Regression\n",
        "- Random Forest\n",
        "- XGBoost\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report,\n",
        "                             roc_auc_score, roc_curve)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import XGBoost, install if not available\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"Note: XGBoost not installed. Will use Logistic Regression and Random Forest only.\")\n",
        "\n",
        "\n",
        "class StockPredictionModel:\n",
        "    \"\"\"Base class for stock prediction models\"\"\"\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.feature_importance = None\n",
        "\n",
        "    def prepare_features(self, df, feature_cols, target_col='Target_Next_Day'):\n",
        "        \"\"\"Prepare features and target for modeling\"\"\"\n",
        "        X = df[feature_cols].copy()\n",
        "        y = df[target_col].copy()\n",
        "\n",
        "        # Handle any remaining NaN values\n",
        "        X = X.fillna(X.mean())\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        self.model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "        return self.model.predict(X_scaled)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Get prediction probabilities\"\"\"\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "        return self.model.predict_proba(X_scaled)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluate model performance\"\"\"\n",
        "        y_pred = self.predict(X_test)\n",
        "        y_pred_proba = self.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        metrics = {\n",
        "            'Accuracy': accuracy_score(y_test, y_pred),\n",
        "            'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "            'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "            'F1-Score': f1_score(y_test, y_pred, zero_division=0),\n",
        "            'ROC-AUC': roc_auc_score(y_test, y_pred_proba)\n",
        "        }\n",
        "\n",
        "        return metrics, y_pred, y_pred_proba\n",
        "\n",
        "\n",
        "def select_features(df):\n",
        "    \"\"\"Select relevant features for modeling\"\"\"\n",
        "    feature_cols = [\n",
        "        # Price features\n",
        "        'Open', 'High', 'Low', 'Close', 'Volume',\n",
        "        'Price_Change_Pct', 'Volatility',\n",
        "\n",
        "        # Momentum features\n",
        "        'Momentum_1d', 'Momentum_3d',\n",
        "\n",
        "        # Moving averages\n",
        "        'MA_5', 'MA_10',\n",
        "\n",
        "        # Volume features\n",
        "        'Volume_Change',\n",
        "\n",
        "        # Sentiment features\n",
        "        'Sentiment_Mean', 'Sentiment_Std', 'News_Count',\n",
        "        'Positive_Mean', 'Negative_Mean',\n",
        "        'Sentiment_Lag1', 'News_Count_Lag1'\n",
        "    ]\n",
        "\n",
        "    # Only keep features that exist in the dataframe\n",
        "    available_features = [col for col in feature_cols if col in df.columns]\n",
        "\n",
        "    return available_features\n",
        "\n",
        "\n",
        "def train_logistic_regression(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train and evaluate Logistic Regression model\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"LOGISTIC REGRESSION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    model = StockPredictionModel(\"Logistic Regression\")\n",
        "    model.model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "    model.train(X_train, y_train)\n",
        "    metrics, y_pred, y_pred_proba = model.evaluate(X_test, y_test)\n",
        "\n",
        "    print(\"\\nPerformance Metrics:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    return model, metrics, y_pred, y_pred_proba\n",
        "\n",
        "\n",
        "def train_random_forest(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train and evaluate Random Forest model\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RANDOM FOREST\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    model = StockPredictionModel(\"Random Forest\")\n",
        "    model.model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    model.train(X_train, y_train)\n",
        "    metrics, y_pred, y_pred_proba = model.evaluate(X_test, y_test)\n",
        "\n",
        "    print(\"\\nPerformance Metrics:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    # Feature importance\n",
        "    model.feature_importance = pd.DataFrame({\n",
        "        'Feature': X_train.columns,\n",
        "        'Importance': model.model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    print(\"\\nTop 5 Important Features:\")\n",
        "    print(model.feature_importance.head().to_string(index=False))\n",
        "\n",
        "    return model, metrics, y_pred, y_pred_proba\n",
        "\n",
        "\n",
        "def train_xgboost(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train and evaluate XGBoost model\"\"\"\n",
        "    if not XGBOOST_AVAILABLE:\n",
        "        print(\"\\nXGBoost not available - skipping\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"XGBOOST\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    model = StockPredictionModel(\"XGBoost\")\n",
        "    model.model = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=3,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "\n",
        "    model.train(X_train, y_train)\n",
        "    metrics, y_pred, y_pred_proba = model.evaluate(X_test, y_test)\n",
        "\n",
        "    print(\"\\nPerformance Metrics:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    # Feature importance\n",
        "    model.feature_importance = pd.DataFrame({\n",
        "        'Feature': X_train.columns,\n",
        "        'Importance': model.model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    print(\"\\nTop 5 Important Features:\")\n",
        "    print(model.feature_importance.head().to_string(index=False))\n",
        "\n",
        "    return model, metrics, y_pred, y_pred_proba\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_test, y_pred, model_name):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Down', 'Up'],\n",
        "                yticklabels=['Down', 'Up'])\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'confusion_matrix_{model_name.replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"✓ Saved confusion_matrix_{model_name.replace(' ', '_')}.png\")\n",
        "\n",
        "\n",
        "def plot_roc_curve(y_test, y_pred_proba, model_name):\n",
        "    \"\"\"Plot ROC curve\"\"\"\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})', linewidth=2)\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curve - {model_name}')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'roc_curve_{model_name.replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"✓ Saved roc_curve_{model_name.replace(' ', '_')}.png\")\n",
        "\n",
        "\n",
        "def plot_feature_importance(model):\n",
        "    \"\"\"Plot feature importance\"\"\"\n",
        "    if model.feature_importance is None:\n",
        "        return\n",
        "\n",
        "    top_features = model.feature_importance.head(10)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
        "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
        "    plt.xlabel('Importance')\n",
        "    plt.title(f'Top 10 Feature Importance - {model.name}')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'feature_importance_{model.name.replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"✓ Saved feature_importance_{model.name.replace(' ', '_')}.png\")\n",
        "\n",
        "\n",
        "def compare_models(results):\n",
        "    \"\"\"Create comparison of all models\"\"\"\n",
        "    comparison_df = pd.DataFrame(results).T\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MODEL COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\n\" + comparison_df.to_string())\n",
        "\n",
        "    # Plot comparison\n",
        "    comparison_df.plot(kind='bar', figsize=(12, 6))\n",
        "    plt.title('Model Performance Comparison')\n",
        "    plt.ylabel('Score')\n",
        "    plt.xlabel('Model')\n",
        "    plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"\\n✓ Saved model_comparison.png\")\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Week 4: Machine Learning Models\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Load ML-ready dataset\n",
        "    try:\n",
        "        df = pd.read_csv('ml_ready_dataset.csv')\n",
        "        print(f\"✓ Loaded dataset with {len(df)} samples\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: ml_ready_dataset.csv not found!\")\n",
        "        print(\"Please run week4_sentiment_features.py first\")\n",
        "        return\n",
        "\n",
        "    # Select features\n",
        "    feature_cols = select_features(df)\n",
        "    print(f\"✓ Selected {len(feature_cols)} features\")\n",
        "\n",
        "    # Prepare data\n",
        "    X = df[feature_cols].fillna(df[feature_cols].mean())\n",
        "    y = df['Target_Next_Day']\n",
        "\n",
        "    # Split data (80-20 split)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining set: {len(X_train)} samples\")\n",
        "    print(f\"Test set: {len(X_test)} samples\")\n",
        "    print(f\"Class distribution: {dict(y.value_counts())}\")\n",
        "\n",
        "    # Train models\n",
        "    results = {}\n",
        "\n",
        "    # 1. Logistic Regression\n",
        "    lr_model, lr_metrics, lr_pred, lr_proba = train_logistic_regression(\n",
        "        X_train, y_train, X_test, y_test\n",
        "    )\n",
        "    results['Logistic Regression'] = lr_metrics\n",
        "    plot_confusion_matrix(y_test, lr_pred, \"Logistic Regression\")\n",
        "    plot_roc_curve(y_test, lr_proba, \"Logistic Regression\")\n",
        "\n",
        "    # 2. Random Forest\n",
        "    rf_model, rf_metrics, rf_pred, rf_proba = train_random_forest(\n",
        "        X_train, y_train, X_test, y_test\n",
        "    )\n",
        "    results['Random Forest'] = rf_metrics\n",
        "    plot_confusion_matrix(y_test, rf_pred, \"Random Forest\")\n",
        "    plot_roc_curve(y_test, rf_proba, \"Random Forest\")\n",
        "    plot_feature_importance(rf_model)\n",
        "\n",
        "    # 3. XGBoost (if available)\n",
        "    if XGBOOST_AVAILABLE:\n",
        "        xgb_model, xgb_metrics, xgb_pred, xgb_proba = train_xgboost(\n",
        "            X_train, y_train, X_test, y_test\n",
        "        )\n",
        "        if xgb_metrics:\n",
        "            results['XGBoost'] = xgb_metrics\n",
        "            plot_confusion_matrix(y_test, xgb_pred, \"XGBoost\")\n",
        "            plot_roc_curve(y_test, xgb_proba, \"XGBoost\")\n",
        "            plot_feature_importance(xgb_model)\n",
        "\n",
        "    # Compare models\n",
        "    comparison_df = compare_models(results)\n",
        "    comparison_df.to_csv('model_comparison_results.csv')\n",
        "    print(\"✓ Saved model_comparison_results.csv\")\n",
        "\n",
        "    # Save best model info\n",
        "    best_model = comparison_df['Accuracy'].idxmax()\n",
        "    best_accuracy = comparison_df['Accuracy'].max()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"BEST MODEL: {best_model}\")\n",
        "    print(f\"Accuracy: {best_accuracy:.4f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\n✓ Week 4 Modeling Complete!\")\n",
        "    print(\"\\nGenerated files:\")\n",
        "    print(\"  - model_comparison_results.csv\")\n",
        "    print(\"  - confusion_matrix_*.png\")\n",
        "    print(\"  - roc_curve_*.png\")\n",
        "    print(\"  - feature_importance_*.png\")\n",
        "    print(\"  - model_comparison.png\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Install required packages if needed\n",
        "    packages = {\n",
        "        'matplotlib': 'matplotlib',\n",
        "        'seaborn': 'seaborn',\n",
        "        'sklearn': 'scikit-learn'\n",
        "    }\n",
        "\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    for import_name, package_name in packages.items():\n",
        "        try:\n",
        "            __import__(import_name)\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package_name}...\")\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package_name])\n",
        "\n",
        "    # Try to install XGBoost\n",
        "    try:\n",
        "        import xgboost\n",
        "    except ImportError:\n",
        "        print(\"Note: XGBoost not installed. Install with: pip install xgboost\")\n",
        "\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fzBEBHDICeE"
      },
      "source": [
        "# Main file for running complete project from dataset creation to testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnT9ZOARkq_2",
        "outputId": "f597ee11-6bbe-41b5-9115-a5344d68726e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "  WiDS 2025 - COMPLETE PROJECT EXECUTION\n",
            "  Stock Volatility Prediction using News Sentiment\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "  INSTALLING PACKAGES\n",
            "======================================================================\n",
            "\n",
            "✓ pandas already installed\n",
            "✓ numpy already installed\n",
            "✓ yfinance already installed\n",
            "✓ requests already installed\n",
            "Installing beautifulsoup4...\n",
            "✓ beautifulsoup4 installed\n",
            "✓ lxml already installed\n",
            "✓ vaderSentiment already installed\n",
            "Installing scikit-learn...\n",
            "✓ scikit-learn installed\n",
            "✓ matplotlib already installed\n",
            "✓ seaborn already installed\n",
            "✓ xgboost already installed\n",
            "\n",
            "======================================================================\n",
            "  STEP 1: DATA COLLECTION (Midterm)\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "WiDS 2025 - Stock Market & News Sentiment Analysis\n",
            "Data Collection Script\n",
            "============================================================\n",
            "\n",
            "Collecting news data from local file 'CAT.rss.xml'...\n",
            "Successfully collected 495 news items from 'CAT.rss.xml'\n",
            "Cleaning news data...\n",
            "Cleaned data: 495 rows with dates from 2025-04-17 to 2026-01-24\n",
            "Collecting stock data for CAT...\n",
            "Successfully collected 1255 days of stock data\n",
            "Merging datasets...\n",
            "Merged dataset: 1524 rows\n",
            "News coverage: 411 days have news\n",
            "\n",
            "Saving datasets...\n",
            "✓ Saved News_raw.csv\n",
            "✓ Saved news_cleaned.csv\n",
            "✓ Saved stock_data.csv\n",
            "✓ Saved merged_medsem_data.csv\n",
            "\n",
            "============================================================\n",
            "DATASET SUMMARY STATISTICS\n",
            "============================================================\n",
            "\n",
            "Date Range: 2021-02-01 to 2026-01-29\n",
            "Total Trading Days: 1524\n",
            "Days with News: 411\n",
            "Days without News: 1113\n",
            "\n",
            "Stock Price Statistics:\n",
            "  Opening Price Range: $154.42 - $655.17\n",
            "  Closing Price Range: $152.95 - $665.24\n",
            "  Average Daily Volume: 2,966,701\n",
            "\n",
            "News Statistics:\n",
            "  Average Headline Length: 77 characters\n",
            "  Shortest Headline: 25 characters\n",
            "  Longest Headline: 221 characters\n",
            "\n",
            "============================================================\n",
            "\n",
            "✓ Data collection complete!\n",
            "\n",
            "Generated files:\n",
            "  - News_raw.csv\n",
            "  - news_cleaned.csv\n",
            "  - stock_data.csv\n",
            "  - merged_medsem_data.csv\n",
            "\n",
            "======================================================================\n",
            "  STEP 2: SENTIMENT ANALYSIS & FEATURE ENGINEERING (Week 4)\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "Week 4: Sentiment Analysis & Feature Engineering\n",
            "============================================================\n",
            "\n",
            "✓ Loaded 495 news items\n",
            "✓ Loaded 1255 trading days\n",
            "Performing sentiment analysis...\n",
            "✓ Sentiment analysis complete for 495 headlines\n",
            "\n",
            "Sentiment Distribution:\n",
            "Sentiment_Label\n",
            "Neutral     234\n",
            "Positive    204\n",
            "Negative     57\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Average Sentiment Score: 0.124\n",
            "\n",
            "Engineering stock features...\n",
            "✓ Engineered 12 new features\n",
            "\n",
            "Creating ML-ready dataset...\n",
            "✓ Created dataset with 1249 samples and 27 features\n",
            "✓ Target distribution: {1.0: 647, 0.0: 602}\n",
            "\n",
            "============================================================\n",
            "FEATURE ENGINEERING SUMMARY\n",
            "============================================================\n",
            "\n",
            "Dataset Shape: (1249, 27)\n",
            "Date Range: 2021-02-08 00:00:00 to 2026-01-28 00:00:00\n",
            "\n",
            "Price Statistics:\n",
            "  Average Daily Return: 0.05%\n",
            "  Volatility (std): 0.0162\n",
            "  Up Days: 647\n",
            "  Down Days: 602\n",
            "\n",
            "Sentiment Statistics:\n",
            "  Average Sentiment: 0.016\n",
            "  Days with News: 142\n",
            "  Average News per Day: 0.3\n",
            "\n",
            "Feature Correlation with Target:\n",
            "  Sentiment_Mean: -0.025\n",
            "  Price_Change_Pct: 0.030\n",
            "  Volatility: -0.028\n",
            "  Momentum_1d: 0.019\n",
            "  Volume_Change: -0.002\n",
            "\n",
            "============================================================\n",
            "\n",
            "Saving processed datasets...\n",
            "✓ Saved news_with_sentiment.csv\n",
            "✓ Saved stock_with_features.csv\n",
            "✓ Saved ml_ready_dataset.csv\n",
            "\n",
            "✓ Week 4 Feature Engineering Complete!\n",
            "\n",
            "Next step: Build prediction models using 'ml_ready_dataset.csv'\n",
            "\n",
            "======================================================================\n",
            "  STEP 3: MACHINE LEARNING MODELING (Week 4)\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "Week 4: Machine Learning Models\n",
            "============================================================\n",
            "\n",
            "✓ Loaded dataset with 1249 samples\n",
            "✓ Selected 19 features\n",
            "\n",
            "Training set: 999 samples\n",
            "Test set: 250 samples\n",
            "Class distribution: {1.0: np.int64(647), 0.0: np.int64(602)}\n",
            "\n",
            "============================================================\n",
            "LOGISTIC REGRESSION\n",
            "============================================================\n",
            "\n",
            "Performance Metrics:\n",
            "  Accuracy: 0.5280\n",
            "  Precision: 0.5323\n",
            "  Recall: 0.7615\n",
            "  F1-Score: 0.6266\n",
            "  ROC-AUC: 0.5169\n",
            "✓ Saved confusion_matrix_Logistic_Regression.png\n",
            "✓ Saved roc_curve_Logistic_Regression.png\n",
            "\n",
            "============================================================\n",
            "RANDOM FOREST\n",
            "============================================================\n",
            "\n",
            "Performance Metrics:\n",
            "  Accuracy: 0.5200\n",
            "  Precision: 0.5305\n",
            "  Recall: 0.6692\n",
            "  F1-Score: 0.5918\n",
            "  ROC-AUC: 0.5408\n",
            "\n",
            "Top 5 Important Features:\n",
            "         Feature  Importance\n",
            "     Momentum_3d    0.102290\n",
            "Price_Change_Pct    0.097929\n",
            "          Volume    0.096093\n",
            "     Momentum_1d    0.091897\n",
            "      Volatility    0.090503\n",
            "✓ Saved confusion_matrix_Random_Forest.png\n",
            "✓ Saved roc_curve_Random_Forest.png\n",
            "✓ Saved feature_importance_Random_Forest.png\n",
            "\n",
            "============================================================\n",
            "XGBOOST\n",
            "============================================================\n",
            "\n",
            "Performance Metrics:\n",
            "  Accuracy: 0.4840\n",
            "  Precision: 0.5037\n",
            "  Recall: 0.5231\n",
            "  F1-Score: 0.5132\n",
            "  ROC-AUC: 0.4761\n",
            "\n",
            "Top 5 Important Features:\n",
            "      Feature  Importance\n",
            "          Low    0.080906\n",
            "Sentiment_Std    0.067910\n",
            "        MA_10    0.067448\n",
            "  Momentum_1d    0.061173\n",
            "         High    0.059444\n",
            "✓ Saved confusion_matrix_XGBoost.png\n",
            "✓ Saved roc_curve_XGBoost.png\n",
            "✓ Saved feature_importance_XGBoost.png\n",
            "\n",
            "============================================================\n",
            "MODEL COMPARISON\n",
            "============================================================\n",
            "\n",
            "                     Accuracy  Precision    Recall  F1-Score   ROC-AUC\n",
            "Logistic Regression     0.528   0.532258  0.761538  0.626582  0.516859\n",
            "Random Forest           0.520   0.530488  0.669231  0.591837  0.540833\n",
            "XGBoost                 0.484   0.503704  0.523077  0.513208  0.476058\n",
            "\n",
            "✓ Saved model_comparison.png\n",
            "✓ Saved model_comparison_results.csv\n",
            "\n",
            "============================================================\n",
            "BEST MODEL: Logistic Regression\n",
            "Accuracy: 0.5280\n",
            "============================================================\n",
            "\n",
            "✓ Week 4 Modeling Complete!\n",
            "\n",
            "Generated files:\n",
            "  - model_comparison_results.csv\n",
            "  - confusion_matrix_*.png\n",
            "  - roc_curve_*.png\n",
            "  - feature_importance_*.png\n",
            "  - model_comparison.png\n",
            "\n",
            "======================================================================\n",
            "  VERIFICATION\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Midterm:\n",
            "  ✓ News_raw.csv\n",
            "  ✓ news_cleaned.csv\n",
            "  ✓ stock_data.csv\n",
            "  ✓ merged_medsem_data.csv\n",
            "\n",
            "Week 4 - Data:\n",
            "  ✓ news_with_sentiment.csv\n",
            "  ✓ stock_with_features.csv\n",
            "  ✓ ml_ready_dataset.csv\n",
            "  ✓ model_comparison_results.csv\n",
            "\n",
            "Week 4 - Visualizations:\n",
            "  ✓ confusion_matrix_Logistic_Regression.png\n",
            "  ✓ confusion_matrix_Random_Forest.png\n",
            "  ✓ roc_curve_Logistic_Regression.png\n",
            "  ✓ roc_curve_Random_Forest.png\n",
            "  ✓ feature_importance_Random_Forest.png\n",
            "  ✓ model_comparison.png\n",
            "\n",
            "Documentation:\n",
            "  ✓ complete_data_collection.py\n",
            "  ✓ week4_sentiment_features.py\n",
            "  ✓ week4_ml_modeling.py\n",
            "  ✓ README.md\n",
            "  ✗ WEEK4_README.md\n",
            "  ✗ requirements.txt\n",
            "\n",
            "⚠ Some files are missing!\n",
            "\n",
            "======================================================================\n",
            "  CREATING SUBMISSION PACKAGE\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✓ Created submission package: WiDS2025_COMPLETE_Submission_20260130/\n",
            "\n",
            "Folder structure:\n",
            "WiDS2025_COMPLETE_Submission_20260130/\n",
            "├── scripts/              (Python scripts)\n",
            "├── data/                 (All CSV files)\n",
            "├── visualizations/       (All PNG plots)\n",
            "├── README.md\n",
            "├── WEEK4_README.md\n",
            "├── requirements.txt\n",
            "└── midterm_submission.ipynb\n",
            "\n",
            "======================================================================\n",
            "  PROJECT SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Files Generated:\n",
            "  CSV Data Files: 19\n",
            "  Visualizations: 28\n",
            "  Python Scripts: 4\n",
            "\n",
            "Midterm Data:\n",
            "  Trading Days: 1255\n",
            "  News Headlines: 495\n",
            "  Date Range: 2021-02-01 to 2026-01-29\n",
            "\n",
            "Week 4 Results:\n",
            "  ML Dataset Size: 1249 samples\n",
            "  Features Used: 26\n",
            "\n",
            "  Best Model Performance:\n",
            "    Model: Logistic Regression\n",
            "    Accuracy: 0.5280\n",
            "    ROC-AUC: 0.5169\n",
            "\n",
            "======================================================================\n",
            "  ✓ PROJECT COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "Total execution time: 36.2 seconds\n",
            "\n",
            "Your submission is ready in: WiDS2025_COMPLETE_Submission_20260130/\n",
            "\n",
            "Next steps:\n",
            "  1. Review the generated visualizations\n",
            "  2. Check model_comparison_results.csv\n",
            "  3. Read WEEK4_README.md for detailed explanations\n",
            "  4. Zip the submission folder\n",
            "  5. Submit to your instructor\n",
            "\n",
            "======================================================================\n",
            "  GOOD LUCK WITH YOUR SUBMISSION!\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "COMPLETE WiDS 2025 PROJECT RUNNER\n",
        "Runs all weeks: Data Collection + Week 4 (Sentiment & ML)\n",
        "\n",
        "Execute this script to generate everything needed for submission\n",
        "\"\"\"\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def print_header(text):\n",
        "    \"\"\"Print formatted header\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"  {text}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "def install_packages():\n",
        "    \"\"\"Install all required packages\"\"\"\n",
        "    print_header(\"INSTALLING PACKAGES\")\n",
        "\n",
        "    packages = [\n",
        "        'pandas', 'numpy',\n",
        "        'yfinance', 'requests', 'beautifulsoup4', 'lxml',\n",
        "        'vaderSentiment',\n",
        "        'scikit-learn', 'matplotlib', 'seaborn',\n",
        "        'xgboost'\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package.replace('-', '_'))\n",
        "            print(f\"✓ {package} already installed\")\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
        "            print(f\"✓ {package} installed\")\n",
        "\n",
        "def run_data_collection():\n",
        "    \"\"\"Run data collection script\"\"\"\n",
        "    print_header(\"STEP 1: DATA COLLECTION (Midterm)\")\n",
        "\n",
        "    try:\n",
        "        from complete_data_collection import main\n",
        "        main()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error in data collection: {e}\")\n",
        "        return False\n",
        "\n",
        "def run_sentiment_analysis():\n",
        "    \"\"\"Run Week 4 sentiment analysis\"\"\"\n",
        "    print_header(\"STEP 2: SENTIMENT ANALYSIS & FEATURE ENGINEERING (Week 4)\")\n",
        "\n",
        "    try:\n",
        "        from week4_sentiment_features import main\n",
        "        main()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error in sentiment analysis: {e}\")\n",
        "        return False\n",
        "\n",
        "def run_ml_modeling():\n",
        "    \"\"\"Run Week 4 machine learning models\"\"\"\n",
        "    print_header(\"STEP 3: MACHINE LEARNING MODELING (Week 4)\")\n",
        "\n",
        "    try:\n",
        "        from week4_ml_modeling import main\n",
        "        main()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error in ML modeling: {e}\")\n",
        "        return False\n",
        "\n",
        "def verify_all_files():\n",
        "    \"\"\"Verify all required files were generated\"\"\"\n",
        "    print_header(\"VERIFICATION\")\n",
        "\n",
        "    required_files = {\n",
        "        'Midterm': [\n",
        "            'News_raw.csv',\n",
        "            'news_cleaned.csv',\n",
        "            'stock_data.csv',\n",
        "            'merged_medsem_data.csv'\n",
        "        ],\n",
        "        'Week 4 - Data': [\n",
        "            'news_with_sentiment.csv',\n",
        "            'stock_with_features.csv',\n",
        "            'ml_ready_dataset.csv',\n",
        "            'model_comparison_results.csv'\n",
        "        ],\n",
        "        'Week 4 - Visualizations': [\n",
        "            'confusion_matrix_Logistic_Regression.png',\n",
        "            'confusion_matrix_Random_Forest.png',\n",
        "            'roc_curve_Logistic_Regression.png',\n",
        "            'roc_curve_Random_Forest.png',\n",
        "            'feature_importance_Random_Forest.png',\n",
        "            'model_comparison.png'\n",
        "        ],\n",
        "        'Documentation': [\n",
        "            'complete_data_collection.py',\n",
        "            'week4_sentiment_features.py',\n",
        "            'week4_ml_modeling.py',\n",
        "            'README.md',\n",
        "            'WEEK4_README.md',\n",
        "            'requirements.txt'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    all_present = True\n",
        "    for category, files in required_files.items():\n",
        "        print(f\"\\n{category}:\")\n",
        "        for file in files:\n",
        "            exists = os.path.exists(file)\n",
        "            status = \"✓\" if exists else \"✗\"\n",
        "            print(f\"  {status} {file}\")\n",
        "            if not exists:\n",
        "                all_present = False\n",
        "\n",
        "    return all_present\n",
        "\n",
        "def create_submission_package():\n",
        "    \"\"\"Create organized submission folder\"\"\"\n",
        "    print_header(\"CREATING SUBMISSION PACKAGE\")\n",
        "\n",
        "    import shutil\n",
        "\n",
        "    folder_name = f\"WiDS2025_COMPLETE_Submission_{datetime.now().strftime('%Y%m%d')}\"\n",
        "\n",
        "    if os.path.exists(folder_name):\n",
        "        shutil.rmtree(folder_name)\n",
        "\n",
        "    os.makedirs(folder_name)\n",
        "    os.makedirs(f\"{folder_name}/visualizations\")\n",
        "    os.makedirs(f\"{folder_name}/data\")\n",
        "    os.makedirs(f\"{folder_name}/scripts\")\n",
        "\n",
        "    # Copy scripts\n",
        "    scripts = [\n",
        "        'complete_data_collection.py',\n",
        "        'week4_sentiment_features.py',\n",
        "        'week4_ml_modeling.py'\n",
        "    ]\n",
        "    for script in scripts:\n",
        "        if os.path.exists(script):\n",
        "            shutil.copy(script, f\"{folder_name}/scripts/\")\n",
        "\n",
        "    # Copy data files\n",
        "    data_files = [\n",
        "        'News_raw.csv', 'news_cleaned.csv', 'stock_data.csv',\n",
        "        'merged_medsem_data.csv', 'news_with_sentiment.csv',\n",
        "        'stock_with_features.csv', 'ml_ready_dataset.csv',\n",
        "        'model_comparison_results.csv'\n",
        "    ]\n",
        "    for file in data_files:\n",
        "        if os.path.exists(file):\n",
        "            shutil.copy(file, f\"{folder_name}/data/\")\n",
        "\n",
        "    # Copy visualizations\n",
        "    import glob\n",
        "    for png_file in glob.glob('*.png'):\n",
        "        shutil.copy(png_file, f\"{folder_name}/visualizations/\")\n",
        "\n",
        "    # Copy documentation\n",
        "    docs = ['README.md', 'WEEK4_README.md', 'requirements.txt']\n",
        "    for doc in docs:\n",
        "        if os.path.exists(doc):\n",
        "            shutil.copy(doc, folder_name)\n",
        "\n",
        "    # Copy notebook if exists\n",
        "    if os.path.exists('midterm_submission.ipynb'):\n",
        "        shutil.copy('midterm_submission.ipynb', folder_name)\n",
        "\n",
        "    print(f\"\\n✓ Created submission package: {folder_name}/\")\n",
        "    print(\"\\nFolder structure:\")\n",
        "    print(f\"{folder_name}/\")\n",
        "    print(\"├── scripts/              (Python scripts)\")\n",
        "    print(\"├── data/                 (All CSV files)\")\n",
        "    print(\"├── visualizations/       (All PNG plots)\")\n",
        "    print(\"├── README.md\")\n",
        "    print(\"├── WEEK4_README.md\")\n",
        "    print(\"├── requirements.txt\")\n",
        "    print(\"└── midterm_submission.ipynb\")\n",
        "\n",
        "    return folder_name\n",
        "\n",
        "def generate_summary():\n",
        "    \"\"\"Generate project summary\"\"\"\n",
        "    print_header(\"PROJECT SUMMARY\")\n",
        "\n",
        "    # Count files\n",
        "    import glob\n",
        "    csv_count = len(glob.glob('*.csv'))\n",
        "    png_count = len(glob.glob('*.png'))\n",
        "    py_count = len(glob.glob('*.py'))\n",
        "\n",
        "    print(\"Files Generated:\")\n",
        "    print(f\"  CSV Data Files: {csv_count}\")\n",
        "    print(f\"  Visualizations: {png_count}\")\n",
        "    print(f\"  Python Scripts: {py_count}\")\n",
        "\n",
        "    # Print data summary if available\n",
        "    try:\n",
        "        import pandas as pd\n",
        "\n",
        "        # Midterm data\n",
        "        stock_df = pd.read_csv('stock_data.csv')\n",
        "        news_df = pd.read_csv('news_cleaned.csv')\n",
        "\n",
        "        print(\"\\nMidterm Data:\")\n",
        "        print(f\"  Trading Days: {len(stock_df)}\")\n",
        "        print(f\"  News Headlines: {len(news_df)}\")\n",
        "        print(f\"  Date Range: {stock_df['Date'].min()} to {stock_df['Date'].max()}\")\n",
        "\n",
        "        # Week 4 data\n",
        "        ml_df = pd.read_csv('ml_ready_dataset.csv')\n",
        "        results_df = pd.read_csv('model_comparison_results.csv', index_col=0)\n",
        "\n",
        "        print(\"\\nWeek 4 Results:\")\n",
        "        print(f\"  ML Dataset Size: {len(ml_df)} samples\")\n",
        "        print(f\"  Features Used: {len(ml_df.columns) - 1}\")\n",
        "        print(\"\\n  Best Model Performance:\")\n",
        "        best_model = results_df['Accuracy'].idxmax()\n",
        "        best_acc = results_df['Accuracy'].max()\n",
        "        print(f\"    Model: {best_model}\")\n",
        "        print(f\"    Accuracy: {best_acc:.4f}\")\n",
        "        print(f\"    ROC-AUC: {results_df.loc[best_model, 'ROC-AUC']:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nCouldn't load summary data: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution\"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"  WiDS 2025 - COMPLETE PROJECT EXECUTION\")\n",
        "    print(\"  Stock Volatility Prediction using News Sentiment\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    # Step 0: Install packages\n",
        "    install_packages()\n",
        "\n",
        "    # Step 1: Data collection (Midterm)\n",
        "    if not run_data_collection():\n",
        "        print(\"\\n⚠ Data collection failed!\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Sentiment analysis (Week 4)\n",
        "    if not run_sentiment_analysis():\n",
        "        print(\"\\n⚠ Sentiment analysis failed!\")\n",
        "        return\n",
        "\n",
        "    # Step 3: ML modeling (Week 4)\n",
        "    if not run_ml_modeling():\n",
        "        print(\"\\n⚠ ML modeling failed!\")\n",
        "        return\n",
        "\n",
        "    # Step 4: Verify all files\n",
        "    if not verify_all_files():\n",
        "        print(\"\\n⚠ Some files are missing!\")\n",
        "\n",
        "    # Step 5: Create submission package\n",
        "    folder_name = create_submission_package()\n",
        "\n",
        "    # Step 6: Generate summary\n",
        "    generate_summary()\n",
        "\n",
        "    # Final message\n",
        "    end_time = datetime.now()\n",
        "    duration = (end_time - start_time).total_seconds()\n",
        "\n",
        "    print_header(\"✓ PROJECT COMPLETE!\")\n",
        "\n",
        "    print(f\"Total execution time: {duration:.1f} seconds\")\n",
        "    print(f\"\\nYour submission is ready in: {folder_name}/\")\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"  1. Review the generated visualizations\")\n",
        "    print(\"  2. Check model_comparison_results.csv\")\n",
        "    print(\"  3. Read WEEK4_README.md for detailed explanations\")\n",
        "    print(\"  4. Zip the submission folder\")\n",
        "    print(\"  5. Submit to your instructor\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  GOOD LUCK WITH YOUR SUBMISSION!\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12985758a2234fe9b7ee8d96a2184d2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "1f3962d0217244a0a1703483a169ea0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12985758a2234fe9b7ee8d96a2184d2e",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87127f8de65646028d58693b8e45cad5",
            "value": 100
          }
        },
        "87127f8de65646028d58693b8e45cad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "black",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
